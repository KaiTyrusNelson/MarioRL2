{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d0b625-45ea-4efb-a49d-baef9fe70e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks\n",
    "from env import train\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5deb8319-2b06-46f1-bd3c-e1342a9b5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 198.8600006610155 average time: 114.85 best_reward: 421.4000016450882\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 191.10500072948633 average time: 112.0 best_reward: 421.300002142787\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 225.9850007493049 average time: 132.0 best_reward: 492.60000213980675\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 218.57000064104795 average time: 113.2 best_reward: 448.1000020876527\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 298.76000080518423 average time: 155.5 best_reward: 596.5000034943223\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 231.70500081516803 average time: 169.2 best_reward: 492.3000031262636\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 283.5850009869784 average time: 142.7 best_reward: 728.6000030115247\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 80000 / 500000\n",
      "average reward: 355.7500010214746 average time: 183.9 best_reward: 984.9000034406781\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 295.7700010530651 average time: 311.2 best_reward: 594.7000013664365\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 221.02000058926643 average time: 104.4 best_reward: 523.3000005930662\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 283.34500107243656 average time: 143.35 best_reward: 728.5000028982759\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 404.22500176765027 average time: 184.25 best_reward: 730.0000026002526\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 327.6800012480468 average time: 201.0 best_reward: 813.700003311038\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 274.3300012178719 average time: 128.65 best_reward: 447.5000020265579\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 279.3850011780858 average time: 128.35 best_reward: 728.0000034123659\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 309.20500142276285 average time: 136.4 best_reward: 576.2000024393201\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 370.3750016152859 average time: 159.1 best_reward: 729.8000039830804\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 433.3150016952306 average time: 189.8 best_reward: 813.700000859797\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 382.3100017402321 average time: 154.45 best_reward: 577.0000022128224\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 429.9250018358231 average time: 194.8 best_reward: 814.0000032931566\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 210000 / 500000\n",
      "average reward: 459.04000177942214 average time: 200.35 best_reward: 986.800004683435\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 424.82000177390876 average time: 177.0 best_reward: 729.0000036805868\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 339.38500126600263 average time: 174.9 best_reward: 726.6000026538968\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 450.85500183068217 average time: 195.0 best_reward: 730.3000023365021\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 305.75000114925206 average time: 175.05 best_reward: 730.0000028535724\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 380.61000158935786 average time: 248.0 best_reward: 730.0000025257468\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 415.74000197276473 average time: 184.35 best_reward: 722.2000040411949\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 391.21000180095433 average time: 195.1 best_reward: 528.4000018313527\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 409.5150017697364 average time: 175.7 best_reward: 814.6000025346875\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 519.7150021038949 average time: 247.45 best_reward: 729.7000021636486\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 581.4500027693808 average time: 247.3 best_reward: 815.2000033780932\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 393.6150016374886 average time: 156.35 best_reward: 731.8000030815601\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 437.70500192902983 average time: 181.2 best_reward: 816.3000039607286\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 340000 / 500000\n",
      "average reward: 524.4050021879375 average time: 230.8 best_reward: 986.4000032320619\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 269.2350011508912 average time: 317.8 best_reward: 525.7000022530556\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 360000 / 500000\n",
      "average reward: 431.6200017519295 average time: 211.0 best_reward: 981.8000031635165\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 370000 / 500000\n",
      "average reward: 529.2400022301823 average time: 289.8 best_reward: 985.9000048264861\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 380000 / 500000\n",
      "average reward: 465.76500178948044 average time: 254.8 best_reward: 987.6000041291118\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 390000 / 500000\n",
      "average reward: 437.58000177741053 average time: 242.05 best_reward: 983.400005504489\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 258.545000968501 average time: 318.4 best_reward: 594.0000024735928\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 410000 / 500000\n",
      "average reward: 364.5800014317036 average time: 299.8 best_reward: 983.3000006973743\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 420000 / 500000\n",
      "average reward: 375.0600014850497 average time: 190.85 best_reward: 986.0000026673079\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 346.8350014910102 average time: 165.1 best_reward: 727.9000037759542\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 440000 / 500000\n",
      "average reward: 382.7850016012788 average time: 212.4 best_reward: 981.9000024348497\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 444.0250021275133 average time: 262.45 best_reward: 866.8000039532781\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 420.45000174678864 average time: 264.6 best_reward: 813.9000031799078\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 470000 / 500000\n",
      "average reward: 446.9650018341839 average time: 271.8 best_reward: 986.60000243783\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 384.00000154264274 average time: 265.75 best_reward: 814.2000033259392\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 490000 / 500000\n",
      "average reward: 392.32000191845 average time: 161.1 best_reward: 986.1000062972307\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 315.3350013256073 average time: 198.05 best_reward: 727.7000033706427\n"
     ]
    }
   ],
   "source": [
    "#train('./None', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9cdd77-05aa-477f-954c-03136e2ef5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 253.91500056572258 average time: 146.2 best_reward: 491.8000001832843\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 224.19500066563486 average time: 123.3 best_reward: 492.5000017359853\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 239.8400007110089 average time: 139.1 best_reward: 421.20000172406435\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 236.54000078402458 average time: 138.55 best_reward: 444.5000018104911\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 238.28500062823295 average time: 139.15 best_reward: 488.5000010058284\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 210.61500063315034 average time: 122.95 best_reward: 415.7000023648143\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 241.9300005033612 average time: 174.5 best_reward: 416.10000060498714\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 172.4950005494058 average time: 87.1 best_reward: 421.6000020056963\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 221.7500006724149 average time: 107.1 best_reward: 448.0000013932586\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 196.42000052966176 average time: 107.5 best_reward: 528.1000023856759\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 249.04500075653195 average time: 123.8 best_reward: 493.800001911819\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 120000 / 500000\n",
      "average reward: 356.2400012176484 average time: 171.8 best_reward: 985.7000029534101\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 258.7300007082522 average time: 121.1 best_reward: 529.6000021696091\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 252.7450009301305 average time: 125.0 best_reward: 448.0000024959445\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 340.92000128030776 average time: 161.8 best_reward: 525.8000015169382\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 347.2450014218688 average time: 157.35 best_reward: 577.8000027909875\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 343.64500135444104 average time: 178.65 best_reward: 728.400002270937\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 180000 / 500000\n",
      "average reward: 415.96000177562235 average time: 180.9 best_reward: 985.7000043541193\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 427.80000192523005 average time: 185.6 best_reward: 726.1000029295683\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 434.5150018531829 average time: 291.65 best_reward: 647.200002759695\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 322.6750013642013 average time: 221.3 best_reward: 647.4000030085444\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 344.2000013325363 average time: 198.7 best_reward: 722.7000036984682\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 290.2200009893626 average time: 157.55 best_reward: 529.700002335012\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 481.0050021670759 average time: 252.95 best_reward: 729.4000034332275\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 361.535001437366 average time: 158.65 best_reward: 596.2000025585294\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 418.8400018505752 average time: 182.85 best_reward: 841.0000039562583\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 417.28500211797655 average time: 171.35 best_reward: 731.3000041022897\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 489.37500204667447 average time: 385.4 best_reward: 731.6000028699636\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 396.90000166222455 average time: 208.65 best_reward: 647.1000030338764\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 455.8200017232448 average time: 247.7 best_reward: 729.0000022873282\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 475.5150020904839 average time: 234.45 best_reward: 757.6000017151237\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 320000 / 500000\n",
      "average reward: 388.95000156164167 average time: 182.25 best_reward: 981.2000034302473\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 330000 / 500000\n",
      "average reward: 459.21000175848604 average time: 218.85 best_reward: 985.3000036627054\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 396.16500189825894 average time: 158.75 best_reward: 730.2000038102269\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 381.3800018388778 average time: 152.9 best_reward: 730.1000043004751\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 381.96000154912474 average time: 276.1 best_reward: 729.7000034600496\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 370000 / 500000\n",
      "average reward: 395.54500166140497 average time: 179.95 best_reward: 984.7000045180321\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 380000 / 500000\n",
      "average reward: 382.6450016785413 average time: 172.3 best_reward: 983.3000051006675\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 390000 / 500000\n",
      "average reward: 502.04000200629235 average time: 230.7 best_reward: 984.3000013232231\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 400000 / 500000\n",
      "average reward: 571.0100021570921 average time: 302.2 best_reward: 985.1000023037195\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 410000 / 500000\n",
      "average reward: 347.4100012883544 average time: 179.15 best_reward: 986.2000037804246\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 420000 / 500000\n",
      "average reward: 516.0650021586567 average time: 236.1 best_reward: 983.9000042304397\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 430000 / 500000\n",
      "average reward: 498.5050021611154 average time: 243.6 best_reward: 984.000003285706\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 440000 / 500000\n",
      "average reward: 379.7600015010685 average time: 181.9 best_reward: 984.5000015646219\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 320.4400012809783 average time: 317.35 best_reward: 722.1000030264258\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 460000 / 500000\n",
      "average reward: 429.60000166222454 average time: 293.35 best_reward: 980.9000033065677\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 470000 / 500000\n",
      "average reward: 412.42500177249315 average time: 208.0 best_reward: 977.0000035911798\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 480000 / 500000\n",
      "average reward: 637.5900028176605 average time: 314.95 best_reward: 985.6000061407685\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 400.7800017055124 average time: 197.9 best_reward: 811.700003258884\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 414.5250017881393 average time: 226.0 best_reward: 814.500004440546\n"
     ]
    }
   ],
   "source": [
    "#train('./Intrins', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, intrins_reward = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67022f3e-1efa-49ca-8d19-026c0eae0bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 288.07000064961613 average time: 219.7 best_reward: 727.5000011846423\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 237.25500032901763 average time: 201.65 best_reward: 414.69999987632036\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 260.11500032693147 average time: 186.95 best_reward: 419.49999929219484\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 240.46000057049096 average time: 139.4 best_reward: 576.6000024750829\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 242.23500066027046 average time: 150.85 best_reward: 524.6000018343329\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 214.16000049486757 average time: 130.65 best_reward: 491.00000102072954\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 218.4900007493794 average time: 105.0 best_reward: 492.00000128149986\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 226.49500074088573 average time: 134.7 best_reward: 447.1000027731061\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 277.64000082351265 average time: 155.35 best_reward: 576.1000031158328\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 252.330000660941 average time: 129.9 best_reward: 595.2000011876225\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 110000 / 500000\n",
      "average reward: 325.2450008101761 average time: 196.65 best_reward: 983.5000007748604\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 215.7100004926324 average time: 120.0 best_reward: 595.9000021889806\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 130000 / 500000\n",
      "average reward: 334.7250009972602 average time: 194.45 best_reward: 984.5000036507845\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 281.0500008951873 average time: 143.35 best_reward: 526.7000021710992\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 289.1450007263571 average time: 152.1 best_reward: 492.6000018417835\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 300.1600008998066 average time: 153.2 best_reward: 594.5000027641654\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 306.990000872314 average time: 152.7 best_reward: 596.400002323091\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 231.60500079020858 average time: 117.15 best_reward: 422.80000173300505\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 269.9900008182973 average time: 129.45 best_reward: 492.70000222325325\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 301.26500116474926 average time: 143.1 best_reward: 528.400002554059\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 351.5150013323873 average time: 157.6 best_reward: 577.5000011473894\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 369.61000138409435 average time: 166.95 best_reward: 602.4000029265881\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 383.25000138171015 average time: 176.05 best_reward: 730.7000034078956\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 357.08000150434674 average time: 151.2 best_reward: 731.1000031083822\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 506.89500222019853 average time: 203.65 best_reward: 815.7000030055642\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 453.04500218331816 average time: 186.55 best_reward: 727.000003375113\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 446.0750020124018 average time: 186.5 best_reward: 815.8000032901764\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 448.1700020618737 average time: 173.85 best_reward: 731.0000034719706\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 482.2400021318346 average time: 196.15 best_reward: 814.3000030145049\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 493.12000240720806 average time: 191.4 best_reward: 731.6000032871962\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 446.6900018937886 average time: 180.05 best_reward: 730.9000029340386\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 355.14000168964265 average time: 136.3 best_reward: 730.9000028222799\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 330000 / 500000\n",
      "average reward: 454.335002014786 average time: 181.05 best_reward: 987.0000037699938\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 481.34500218145547 average time: 186.95 best_reward: 811.3000031933188\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 350000 / 500000\n",
      "average reward: 464.1800020672381 average time: 180.2 best_reward: 985.500003054738\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 360000 / 500000\n",
      "average reward: 414.84000174477694 average time: 173.4 best_reward: 986.1000047326088\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 464.41000209972265 average time: 186.5 best_reward: 810.0000033825636\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 384.365001751855 average time: 160.6 best_reward: 730.000003516674\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 600.5200026821345 average time: 244.15 best_reward: 815.6000039055943\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 400000 / 500000\n",
      "average reward: 555.4950022131204 average time: 228.1 best_reward: 986.7000020891428\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 410000 / 500000\n",
      "average reward: 532.5200022328645 average time: 221.95 best_reward: 986.8000013232231\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 420000 / 500000\n",
      "average reward: 553.8000023368746 average time: 230.6 best_reward: 987.100003734231\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 405.7700018800795 average time: 152.1 best_reward: 596.8000030368567\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 464.1250019669533 average time: 205.05 best_reward: 808.7000027820468\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 550.1650025952607 average time: 219.85 best_reward: 815.6000042557716\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 456.83000181466343 average time: 191.7 best_reward: 815.5000028833747\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 470000 / 500000\n",
      "average reward: 540.9050022851676 average time: 216.55 best_reward: 986.0000031739473\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 468.7250018998981 average time: 216.55 best_reward: 814.4000033438206\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 490000 / 500000\n",
      "average reward: 546.9950025964529 average time: 219.95 best_reward: 986.500003054738\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 488.4350020624697 average time: 205.35 best_reward: 814.40000333637\n"
     ]
    }
   ],
   "source": [
    "#train('./Annealing', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, annealing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cbb92a4-a5be-49d8-8940-33292e55ce74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 231.8850003749132 average time: 189.55 best_reward: 487.0000003054738\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 249.02000081017613 average time: 136.75 best_reward: 492.20000091940165\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 245.8550007712096 average time: 137.25 best_reward: 524.600001975894\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 323.99000112600623 average time: 182.85 best_reward: 492.20000103861094\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 210.9550007622689 average time: 96.6 best_reward: 447.30000127106905\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 222.15500076636673 average time: 101.0 best_reward: 577.3000015467405\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 249.3450008071959 average time: 120.0 best_reward: 492.9000024497509\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 409.9150015767664 average time: 180.3 best_reward: 728.6000029593706\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 307.09500102065505 average time: 147.95 best_reward: 576.4000015854836\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 461.6900016833097 average time: 201.5 best_reward: 729.500003091991\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 404.42000161260364 average time: 177.2 best_reward: 594.6000007092953\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 120000 / 500000\n",
      "average reward: 433.7500017017126 average time: 202.2 best_reward: 983.0000004470348\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 465.1800020992756 average time: 196.75 best_reward: 729.6000025868416\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 449.28500206544993 average time: 193.6 best_reward: 814.7000042274594\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 150000 / 500000\n",
      "average reward: 316.6350011661649 average time: 136.35 best_reward: 985.7000049129128\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 399.56000176519154 average time: 165.45 best_reward: 731.0000027641654\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 170000 / 500000\n",
      "average reward: 519.5700022466481 average time: 248.3 best_reward: 987.4000021219254\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 180000 / 500000\n",
      "average reward: 490.4700018823147 average time: 273.6 best_reward: 986.7000025585294\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 190000 / 500000\n",
      "average reward: 567.9600022938102 average time: 252.1 best_reward: 986.7000036984682\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 200000 / 500000\n",
      "average reward: 377.05500145182015 average time: 154.4 best_reward: 985.9000054076314\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 210000 / 500000\n",
      "average reward: 547.9800025474281 average time: 234.65 best_reward: 986.500006057322\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 220000 / 500000\n",
      "average reward: 617.3050024610013 average time: 356.7 best_reward: 987.3000031560659\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 230000 / 500000\n",
      "average reward: 333.2950013950467 average time: 177.75 best_reward: 986.4000043869019\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 264.26000119559467 average time: 134.8 best_reward: 729.3000025600195\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 250000 / 500000\n",
      "average reward: 394.8500021502376 average time: 230.9 best_reward: 978.1000060364604\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 260000 / 500000\n",
      "average reward: 446.1850015502423 average time: 346.55 best_reward: 977.799999922514\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 270000 / 500000\n",
      "average reward: 591.1850021705031 average time: 418.25 best_reward: 984.600002489984\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 280000 / 500000\n",
      "average reward: 573.1600022483617 average time: 322.05 best_reward: 981.3000022172928\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 290000 / 500000\n",
      "average reward: 594.8600027736277 average time: 255.55 best_reward: 986.9000039696693\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 300000 / 500000\n",
      "average reward: 505.2750021297485 average time: 239.95 best_reward: 986.500002630055\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 310000 / 500000\n",
      "average reward: 430.42000175751747 average time: 185.75 best_reward: 986.5000027343631\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 320000 / 500000\n",
      "average reward: 633.9150026287883 average time: 279.7 best_reward: 986.2000050246716\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 446.745002085343 average time: 256.85 best_reward: 810.2000031396747\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 340000 / 500000\n",
      "average reward: 635.2750029820949 average time: 284.25 best_reward: 986.1000023186207\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 350000 / 500000\n",
      "average reward: 507.4850022062659 average time: 336.1 best_reward: 986.4000033140182\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 360000 / 500000\n",
      "average reward: 508.515002143383 average time: 411.9 best_reward: 987.0000045150518\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 582.0750028755516 average time: 292.7 best_reward: 814.9000035449862\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 380000 / 500000\n",
      "average reward: 544.23000237979 average time: 225.35 best_reward: 985.8000036403537\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 390000 / 500000\n",
      "average reward: 578.7350027877837 average time: 240.8 best_reward: 985.9000052511692\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 284.62500145249066 average time: 285.7 best_reward: 727.8000036627054\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 367.0650017891079 average time: 236.1 best_reward: 721.9000029563904\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 420000 / 500000\n",
      "average reward: 618.5600031677634 average time: 254.0 best_reward: 985.9000053852797\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 430000 / 500000\n",
      "average reward: 621.6850028775632 average time: 258.35 best_reward: 986.7000060677528\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 440000 / 500000\n",
      "average reward: 549.2850022230298 average time: 263.6 best_reward: 986.6000032946467\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 450000 / 500000\n",
      "average reward: 650.9600029785186 average time: 296.6 best_reward: 987.5000056475401\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 460000 / 500000\n",
      "average reward: 680.0450029108673 average time: 294.3 best_reward: 988.1000053733587\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 470000 / 500000\n",
      "average reward: 626.8150028724224 average time: 335.8 best_reward: 987.4000047445297\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 480000 / 500000\n",
      "average reward: 838.4450035192073 average time: 369.9 best_reward: 987.2000064477324\n",
      "GOAL\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 490000 / 500000\n",
      "average reward: 471.77500228099524 average time: 492.1 best_reward: 983.200003914535\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 438.5550016835332 average time: 492.05 best_reward: 870.1000035703182\n"
     ]
    }
   ],
   "source": [
    "#train('./ValueClip', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, value_clip = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a87f26-61fd-401d-a8d2-94b6ea8062ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('./DogWaterModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0a20fc-8330-42ac-8bd9-3e6a0b1377c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 173.18000011518598 average time: 279.5 best_reward: 259.8000006452203\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 261.5950005244464 average time: 393.05 best_reward: 486.5000009909272\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 243.4550005823374 average time: 435.9 best_reward: 480.80000112205744\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 177.2300003156066 average time: 464.2 best_reward: 253.40000077337027\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 229.73000053949653 average time: 300.9 best_reward: 440.40000151097775\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 208.33000042624772 average time: 248.65 best_reward: 326.8999999240041\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 198.72000039741397 average time: 479.85 best_reward: 410.2000001966953\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 192.4550003003329 average time: 165.4 best_reward: 414.9999999254942\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 240.68500054888426 average time: 267.65 best_reward: 527.5000018030405\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 249.89500053115188 average time: 228.85 best_reward: 724.2000013589859\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 326.2800006389618 average time: 208.8 best_reward: 592.4000023901463\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 268.8500005979091 average time: 190.9 best_reward: 594.2000019848347\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 237.78500034362077 average time: 174.3 best_reward: 490.9000015631318\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 249.98000071309508 average time: 148.4 best_reward: 525.0000023394823\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 237.36500038169325 average time: 148.55 best_reward: 417.5000008121133\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 250.51000062003732 average time: 127.45 best_reward: 529.9000017866492\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 170000 / 500000\n",
      "average reward: 328.48500082567335 average time: 205.55 best_reward: 980.5000050589442\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 273.8400009475648 average time: 157.8 best_reward: 517.3000026866794\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 263.1800006058067 average time: 137.75 best_reward: 421.20000045001507\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 266.9000007811934 average time: 154.8 best_reward: 489.6000000461936\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 280.035000616312 average time: 151.85 best_reward: 448.4000018015504\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 207.68500059582294 average time: 118.4 best_reward: 420.5000018104911\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 274.9150007311255 average time: 178.05 best_reward: 858.1000023111701\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 252.71000082306563 average time: 139.6 best_reward: 421.5000014305115\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 256.885000840202 average time: 125.95 best_reward: 530.1000024452806\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 220.94500056318938 average time: 124.7 best_reward: 420.4000016003847\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 283.8150007832795 average time: 147.55 best_reward: 422.70000134408474\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 271.150000718981 average time: 144.75 best_reward: 490.8000004813075\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 320.3750008098781 average time: 162.8 best_reward: 728.3000018596649\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 250.51500053554773 average time: 137.75 best_reward: 447.5000017285347\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 244.1500006586313 average time: 133.0 best_reward: 525.4000023975968\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 216.07000078074634 average time: 115.1 best_reward: 534.6000017672777\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 230.1700007136911 average time: 112.2 best_reward: 526.8000006303191\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 285.7900006607175 average time: 153.65 best_reward: 526.3000015765429\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 256.40000088177624 average time: 129.6 best_reward: 525.2000025734305\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 282.1550007518381 average time: 144.45 best_reward: 576.1000012978911\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 257.99500074721874 average time: 135.75 best_reward: 443.0000018328428\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 280.8050008870661 average time: 152.8 best_reward: 577.0000011771917\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 198.43500073365868 average time: 99.0 best_reward: 447.3000019490719\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 220.07500084452332 average time: 114.7 best_reward: 419.00000121444464\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 233.27000055611134 average time: 119.1 best_reward: 493.00000187009573\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 216.55000061541796 average time: 104.15 best_reward: 577.4000013768673\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 251.6650007277727 average time: 118.4 best_reward: 493.6000020131469\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 440000 / 500000\n",
      "average reward: 294.09000086411834 average time: 162.8 best_reward: 983.0000026002526\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 309.785000917688 average time: 169.3 best_reward: 728.8000035136938\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 227.5050006262958 average time: 114.9 best_reward: 447.10000232607126\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 470000 / 500000\n",
      "average reward: 261.5800006330013 average time: 128.2 best_reward: 492.4000016152859\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 480000 / 500000\n",
      "average reward: 296.96500071696937 average time: 150.3 best_reward: 985.1000001356006\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 253.82000084109603 average time: 126.45 best_reward: 728.9000039920211\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 261.5500008635223 average time: 133.05 best_reward: 529.800002463162\n"
     ]
    }
   ],
   "source": [
    "train('./VNet[64,64]', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, arch=[64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88892a31-cef6-4cc2-8b2c-1c604b55f503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 207.95500070527197 average time: 115.5 best_reward: 419.6000012680888\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 256.79000087827444 average time: 129.65 best_reward: 575.4000009074807\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 30000 / 500000\n",
      "average reward: 288.2800005801022 average time: 201.45 best_reward: 979.0000011846423\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 288.3150007288903 average time: 183.3 best_reward: 524.4000019803643\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 200.5550004079938 average time: 123.4 best_reward: 422.00000098347664\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 216.8200006082654 average time: 137.1 best_reward: 444.2000008299947\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 250.2700007099658 average time: 132.05 best_reward: 492.10000175982714\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 213.33000051118432 average time: 123.95 best_reward: 491.9000026807189\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 254.21500071212648 average time: 139.75 best_reward: 728.500002682209\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 263.0700006943196 average time: 137.25 best_reward: 420.400001116097\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 201.94500058367848 average time: 100.9 best_reward: 729.0000024363399\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 179.64000054039062 average time: 93.05 best_reward: 529.8000011816621\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 226.69500061534345 average time: 127.3 best_reward: 523.8000020831823\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 270.5450005616993 average time: 175.55 best_reward: 527.6000021845102\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 251.46000061966478 average time: 133.05 best_reward: 493.00000186264515\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 264.7700008146465 average time: 131.7 best_reward: 492.10000156611204\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 215.26500055342913 average time: 107.55 best_reward: 492.50000208616257\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 245.23500073812903 average time: 137.45 best_reward: 493.30000139027834\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 215.16500069387257 average time: 104.7 best_reward: 448.90000253915787\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 254.31500076651574 average time: 146.1 best_reward: 813.6000013276935\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 238.67500058822333 average time: 137.55 best_reward: 447.0000017955899\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 265.46000070236624 average time: 145.2 best_reward: 529.3000023663044\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 255.11000076532363 average time: 130.15 best_reward: 575.300001449883\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 290.3700007684529 average time: 152.4 best_reward: 529.6000019088387\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 206.99500076361 average time: 105.2 best_reward: 421.20000164955854\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 260000 / 500000\n",
      "average reward: 246.23500065281988 average time: 124.3 best_reward: 983.0000001937151\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 249.3800005313009 average time: 131.2 best_reward: 492.2000022009015\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 211.91000048108398 average time: 134.35 best_reward: 420.200000166893\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 203.30000056512654 average time: 94.6 best_reward: 421.7000011205673\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 251.10000055767597 average time: 144.45 best_reward: 416.80000028014183\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 240.70000052303075 average time: 134.0 best_reward: 418.3000012487173\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 226.65000064224006 average time: 109.25 best_reward: 730.1000026538968\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 231.6800008494407 average time: 113.2 best_reward: 525.1000019684434\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 213.56500043831767 average time: 115.6 best_reward: 577.5000026747584\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 260.235000821203 average time: 147.5 best_reward: 446.40000204741955\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 290.7750009037554 average time: 151.65 best_reward: 577.4000013098121\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 250.61000071018935 average time: 133.1 best_reward: 728.4000013545156\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 315.345000821352 average time: 155.2 best_reward: 576.4000013396144\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 259.4550005991012 average time: 151.45 best_reward: 448.3000013977289\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 263.81000086404384 average time: 135.9 best_reward: 528.6000008434057\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 304.74500081688166 average time: 162.05 best_reward: 524.5000022947788\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 251.79000069089233 average time: 130.15 best_reward: 444.80000177025795\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 247.97500079534947 average time: 132.7 best_reward: 526.500001899898\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 202.45000070892274 average time: 100.5 best_reward: 488.90000239759684\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 283.4800008755177 average time: 138.3 best_reward: 447.9000019952655\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 243.53500077053906 average time: 119.15 best_reward: 527.500001296401\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 470000 / 500000\n",
      "average reward: 295.1600009121001 average time: 154.15 best_reward: 595.600001744926\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 263.0700007904321 average time: 140.95 best_reward: 447.60000202059746\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 262.29500075690447 average time: 120.4 best_reward: 493.8000020161271\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 323.0950011450797 average time: 155.0 best_reward: 727.6000027135015\n"
     ]
    }
   ],
   "source": [
    "train('./VNet[32,32,32,32]', activation_function = torch.nn.Tanh, grad_norm = 0.5, orthagonal_init=True, arch=[32, 32, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b05c153-f7bd-41b7-8ec8-1af7a5654e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 245.2950005337596 average time: 269.0 best_reward: 511.70000094920397\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 20000 / 500000\n",
      "average reward: 289.04000062793494 average time: 271.85 best_reward: 978.2000025436282\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 252.87000058107077 average time: 222.1 best_reward: 569.2000020816922\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 212.68500047028064 average time: 301.0 best_reward: 327.1000012084842\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 295.46000070124865 average time: 270.25 best_reward: 726.2000005766749\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 224.8600003849715 average time: 179.25 best_reward: 725.5000002533197\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 239.28500054478644 average time: 190.2 best_reward: 438.5999996587634\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 210.45500038824974 average time: 128.7 best_reward: 420.3000019341707\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 259.52500072531404 average time: 132.2 best_reward: 531.800002194941\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 271.73000083975495 average time: 154.4 best_reward: 729.9000034630299\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 240.51000066325068 average time: 124.85 best_reward: 492.40000224113464\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 219.53500062562526 average time: 105.95 best_reward: 491.60000117868185\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 130000 / 500000\n",
      "average reward: 286.0300008490682 average time: 152.15 best_reward: 984.0000035911798\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 189.95000052526592 average time: 90.55 best_reward: 527.8000024184585\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 235.59000053219498 average time: 115.25 best_reward: 448.4000022113323\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 219.40000069588422 average time: 106.45 best_reward: 492.6000009700656\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 265.5300007238984 average time: 141.2 best_reward: 447.6000018119812\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 237.41500096842645 average time: 113.6 best_reward: 528.0000020340085\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 245.5750005956739 average time: 142.6 best_reward: 524.9000002965331\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 251.56500077322124 average time: 130.75 best_reward: 593.5000018402934\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 242.45000081323087 average time: 127.95 best_reward: 446.60000216960907\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 199.09000046662987 average time: 102.95 best_reward: 492.1000013947487\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 187.67000027559698 average time: 106.1 best_reward: 413.80000100284815\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 214.77500050440432 average time: 102.6 best_reward: 448.1000006124377\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 208.6950004156679 average time: 111.15 best_reward: 448.5000006556511\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 303.8700009837747 average time: 157.5 best_reward: 594.900001950562\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 200.92500062584878 average time: 100.55 best_reward: 492.70000184327364\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 232.43500062562526 average time: 126.05 best_reward: 443.9000023007393\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 246.7950007468462 average time: 137.05 best_reward: 419.7000018134713\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 247.48500074334441 average time: 121.35 best_reward: 524.1000012531877\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 222.30500054620205 average time: 122.8 best_reward: 491.80000226199627\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 261.5100009586662 average time: 128.45 best_reward: 582.6000014841557\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 275.00000069327655 average time: 146.75 best_reward: 528.8000012785196\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 294.9100009355694 average time: 155.15 best_reward: 814.2000034153461\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 226.87000068947674 average time: 119.65 best_reward: 491.9000019878149\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 201.03000059872866 average time: 108.3 best_reward: 420.30000073462725\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 211.83500072062014 average time: 115.85 best_reward: 421.80000145733356\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 380000 / 500000\n",
      "average reward: 306.19000108502803 average time: 165.25 best_reward: 983.8000031709671\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 226.285000577569 average time: 130.6 best_reward: 728.1000013202429\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 223.0600006263703 average time: 123.3 best_reward: 493.0000015795231\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 271.3400006365031 average time: 139.9 best_reward: 448.7000020444393\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 295.70000078827144 average time: 160.4 best_reward: 493.50000187009573\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 278.23000080771743 average time: 161.25 best_reward: 576.4000017642975\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 252.0050005685538 average time: 130.75 best_reward: 595.9000005424023\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 353.40000093206766 average time: 181.85 best_reward: 596.4000025764108\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 218.00500074885787 average time: 116.9 best_reward: 493.10000175982714\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 470000 / 500000\n",
      "average reward: 212.60000053048134 average time: 113.25 best_reward: 421.4000013694167\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 256.8950007259846 average time: 134.3 best_reward: 572.3000011220574\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 271.49000077098606 average time: 139.3 best_reward: 447.9000021442771\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 283.18000096678736 average time: 143.55 best_reward: 448.100002579391\n"
     ]
    }
   ],
   "source": [
    "train('./VNet[64,64,64,64]', activation_function = torch.nn.Tanh, grad_norm = 0.5, orthagonal_init=True, arch=[64, 64, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ccb777a-704d-4fac-be8f-d45e44226950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 227.5650004874915 average time: 213.35 best_reward: 585.9000025242567\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 265.975000404194 average time: 317.8 best_reward: 575.4000008404255\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 198.7950004402548 average time: 147.8 best_reward: 411.3000011742115\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 257.1700006406754 average time: 214.2 best_reward: 489.9000018686056\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 276.52500048168 average time: 246.3 best_reward: 446.9000020623207\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 288.1800007570535 average time: 220.6 best_reward: 521.2000013738871\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 70000 / 500000\n",
      "average reward: 291.68000058084726 average time: 188.0 best_reward: 983.2000015825033\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 179.34500042162836 average time: 100.35 best_reward: 364.80000108480453\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 251.2700006816536 average time: 139.8 best_reward: 449.0000007972121\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 288.41000088378786 average time: 156.3 best_reward: 534.700001321733\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 233.24000054709614 average time: 122.5 best_reward: 448.6000016257167\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 230.26500066183507 average time: 119.2 best_reward: 529.6000019237399\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 245.2650003861636 average time: 160.65 best_reward: 420.7000003159046\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 242.56500055007638 average time: 138.45 best_reward: 595.100002206862\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 190.3450004581362 average time: 98.5 best_reward: 448.5000018849969\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 276.1800006732345 average time: 153.55 best_reward: 814.4000019058585\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 233.03000073768197 average time: 124.05 best_reward: 420.8000016286969\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 232.3250004801899 average time: 136.35 best_reward: 418.90000220388174\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 261.19000062979757 average time: 153.85 best_reward: 525.8000022396445\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 211.35000037401915 average time: 127.65 best_reward: 413.7000010237098\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 181.13500043302776 average time: 100.8 best_reward: 421.1000021472573\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 281.35500079914925 average time: 151.7 best_reward: 493.3000023290515\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 280.34000078476964 average time: 161.45 best_reward: 448.0000019520521\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 240.4750007059425 average time: 129.45 best_reward: 728.0000022575259\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 245.0550009354949 average time: 119.15 best_reward: 596.7000032737851\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 273.92500066831707 average time: 165.65 best_reward: 596.9000015705824\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 331.700000776723 average time: 204.05 best_reward: 813.100001566112\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 268.07500053718684 average time: 156.05 best_reward: 448.9000009149313\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 256.47000067681074 average time: 137.75 best_reward: 575.5000017136335\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 240.99500071145593 average time: 131.95 best_reward: 447.20000173151493\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 242.04000073745846 average time: 132.55 best_reward: 529.4000026285648\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 203.13000059872866 average time: 107.0 best_reward: 447.80000173300505\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 239.05000067725777 average time: 134.45 best_reward: 595.4000020325184\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 340000 / 500000\n",
      "average reward: 281.6100007712841 average time: 137.3 best_reward: 985.0000031739473\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 227.2150007236749 average time: 133.85 best_reward: 422.40000136196613\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 240.80500068366527 average time: 129.05 best_reward: 421.3000011071563\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 232.51500062271953 average time: 121.6 best_reward: 530.0000020712614\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 234.37500054277479 average time: 123.6 best_reward: 447.7000014334917\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 277.3550007957965 average time: 149.45 best_reward: 494.3000015541911\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 284.96000089347365 average time: 149.45 best_reward: 529.8000025823712\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 246.56500076279045 average time: 127.85 best_reward: 490.9000002220273\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 265.2650007724762 average time: 146.85 best_reward: 446.80000103265047\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 208.6500006016344 average time: 114.65 best_reward: 422.8000020161271\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 239.10000061765314 average time: 125.15 best_reward: 529.7000012844801\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 203.99500061161817 average time: 107.65 best_reward: 420.2000015601516\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 257.0950004316866 average time: 147.95 best_reward: 447.9000013023615\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 470000 / 500000\n",
      "average reward: 297.0150007143617 average time: 148.45 best_reward: 524.5000009387732\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 234.4950005557388 average time: 120.4 best_reward: 421.50000067055225\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 272.5650007579476 average time: 151.15 best_reward: 491.50000189989805\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 244.24500032179057 average time: 142.95 best_reward: 528.3000007942319\n"
     ]
    }
   ],
   "source": [
    "train('./NoneExtractor2', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, feature_extractor = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad54c9c-948b-40d2-908d-2c6aceeaf9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 219.63500059992074 average time: 179.3 best_reward: 356.6999997422099\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 202.3800007712096 average time: 108.55 best_reward: 421.9000006765127\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 206.72000068128108 average time: 125.95 best_reward: 726.2000026777387\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 165.18000051267444 average time: 94.1 best_reward: 523.7000019773841\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 226.60500067807735 average time: 139.35 best_reward: 419.0000016093254\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 232.8700008813292 average time: 125.45 best_reward: 528.6000013574958\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 245.98000070266426 average time: 164.3 best_reward: 726.1000029966235\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 238.04500062130393 average time: 130.05 best_reward: 524.000000692904\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 171.69000042639672 average time: 90.85 best_reward: 419.2000010833144\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 183.6400005992502 average time: 106.05 best_reward: 490.2000020071864\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 212.3100003942847 average time: 116.1 best_reward: 421.40000100433826\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 241.03000084161758 average time: 119.6 best_reward: 525.4000021740794\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 195.33500046655536 average time: 105.35 best_reward: 446.7000013515353\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 287.6900005567819 average time: 159.7 best_reward: 812.6000006124377\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 324.6250007405877 average time: 183.1 best_reward: 813.9000028371811\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 232.505000622198 average time: 110.0 best_reward: 728.6000023707747\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 209.2900006622076 average time: 105.35 best_reward: 729.2000007480383\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 231.6900008559227 average time: 111.3 best_reward: 597.4000027626753\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 194.64500062130392 average time: 96.4 best_reward: 329.6000007838011\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 206.2000006444752 average time: 99.05 best_reward: 448.2000021710992\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 224.63500074595214 average time: 111.1 best_reward: 413.8000005632639\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 290.55500099845233 average time: 143.25 best_reward: 492.6000030115247\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 247.7650006387383 average time: 123.45 best_reward: 493.4000021368265\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 279.8000008393079 average time: 146.8 best_reward: 492.4000026360154\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 254.42500070184468 average time: 130.4 best_reward: 448.40000227838755\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 216.51500066146255 average time: 101.15 best_reward: 421.0000016465783\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 256.185000757128 average time: 127.95 best_reward: 444.60000095516443\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 353.11000129655 average time: 173.25 best_reward: 813.900002092123\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 290000 / 500000\n",
      "average reward: 245.99000088833273 average time: 111.55 best_reward: 985.7000043988228\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 292.68500113487244 average time: 131.9 best_reward: 577.700002476573\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 237.8650006234646 average time: 112.45 best_reward: 492.40000250935555\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 283.0800011042505 average time: 130.5 best_reward: 596.9000031650066\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 297.64000123143194 average time: 136.55 best_reward: 493.1000020727515\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 281.81000079885126 average time: 127.35 best_reward: 596.4000017940998\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 261.67000085823236 average time: 121.35 best_reward: 489.7000025212765\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 318.67000127621 average time: 148.2 best_reward: 525.3000024557114\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 358.1300012044609 average time: 167.3 best_reward: 597.9000029265881\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 266.5050008725375 average time: 123.0 best_reward: 577.5000022947788\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 293.6900008685887 average time: 151.9 best_reward: 728.900001630187\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 294.59500098824503 average time: 141.4 best_reward: 576.5000030770898\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 394.92500167787074 average time: 184.45 best_reward: 730.4000022634864\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 256.9750008173287 average time: 117.3 best_reward: 573.5000020265579\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 283.8000010877848 average time: 120.1 best_reward: 529.700002387166\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 337.09000107832253 average time: 156.3 best_reward: 600.9000018462539\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 450000 / 500000\n",
      "average reward: 297.33000106438993 average time: 135.15 best_reward: 985.000001296401\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 279.6700009837747 average time: 126.75 best_reward: 728.9000019654632\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 470000 / 500000\n",
      "average reward: 320.1350008945912 average time: 140.45 best_reward: 986.8000035881996\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 480000 / 500000\n",
      "average reward: 364.28000144287944 average time: 163.0 best_reward: 986.0000045150518\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 310.6800011169165 average time: 144.6 best_reward: 577.6000030115247\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 311.78500105701386 average time: 138.4 best_reward: 576.1000020131469\n"
     ]
    }
   ],
   "source": [
    "train('./NoneExtractor3', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5,feature_extractor = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5882e70-56c3-4748-8383-b95c43148c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 257.75000074729326 average time: 242.3 best_reward: 490.7000028267503\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 217.6450003363192 average time: 228.8 best_reward: 526.2000024169683\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 251.15500066541136 average time: 179.45 best_reward: 489.90000104904175\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 242.61500064842403 average time: 147.85 best_reward: 491.80000199377537\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 237.23000080399214 average time: 132.65 best_reward: 523.9000019207597\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 268.3600005470216 average time: 185.25 best_reward: 420.9000017642975\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 227.23000066876412 average time: 121.65 best_reward: 447.6000014692545\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 193.10500054769219 average time: 100.55 best_reward: 446.8000023961067\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 265.3100007805973 average time: 137.8 best_reward: 492.60000222176313\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 230.61000062189996 average time: 125.05 best_reward: 492.0000016465783\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 204.985000590235 average time: 113.8 best_reward: 492.0000007599592\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 241.60000087767838 average time: 121.7 best_reward: 447.60000244528055\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 272.7950006771833 average time: 148.5 best_reward: 525.7000019028783\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 246.2550007980317 average time: 145.7 best_reward: 572.7000015452504\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 228.98500042520465 average time: 140.2 best_reward: 488.50000035762787\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 209.99500065669417 average time: 108.95 best_reward: 419.8000010550022\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 214.20500041656197 average time: 130.25 best_reward: 419.40000154078007\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 250.7650008250028 average time: 161.25 best_reward: 813.600003361702\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 190000 / 500000\n",
      "average reward: 234.01000082716345 average time: 139.85 best_reward: 984.7000063285232\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 225.920000866428 average time: 122.8 best_reward: 577.2000022754073\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 205.26500060036778 average time: 107.75 best_reward: 527.1000011712313\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 208.91000039987267 average time: 117.45 best_reward: 446.9000018015504\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 269.3200006324798 average time: 176.25 best_reward: 490.3000016063452\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 265.945000576973 average time: 158.95 best_reward: 529.2999997660518\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 286.86000055633485 average time: 175.6 best_reward: 571.9000020027161\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 289.5950004979968 average time: 192.15 best_reward: 720.4000004455447\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 220.54500035904348 average time: 136.85 best_reward: 524.6000006943941\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 291.34000066034497 average time: 220.55 best_reward: 593.1000024601817\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 305.025000410527 average time: 195.3 best_reward: 727.8000000566244\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 300000 / 500000\n",
      "average reward: 302.1900010060519 average time: 162.6 best_reward: 982.7000025957823\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 235.2450004581362 average time: 156.2 best_reward: 489.80000118911266\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 262.82500049471855 average time: 162.05 best_reward: 571.5000019222498\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 224.6750007186085 average time: 132.35 best_reward: 419.100001655519\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 259.1400004860014 average time: 164.1 best_reward: 487.8000011816621\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 189.1950005799532 average time: 98.3 best_reward: 576.5000017806888\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 231.7800005789846 average time: 130.95 best_reward: 490.70000088214874\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 236.23500046879053 average time: 135.85 best_reward: 447.80000115931034\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 234.33000048398972 average time: 137.5 best_reward: 524.2000019028783\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 213.31500061042607 average time: 114.35 best_reward: 487.10000067204237\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 400000 / 500000\n",
      "average reward: 280.06500070914626 average time: 161.4 best_reward: 985.6000041142106\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 234.44000061452388 average time: 135.1 best_reward: 595.4000006094575\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 275.03500078991055 average time: 155.85 best_reward: 524.4000015109777\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 232.25000051669775 average time: 161.35 best_reward: 420.90000170469284\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 244.22000035196544 average time: 155.75 best_reward: 446.90000155568123\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "GOAL\n",
      "time steps: 450000 / 500000\n",
      "average reward: 313.98000089116397 average time: 165.85 best_reward: 985.6000030636787\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 226.9350005108863 average time: 134.85 best_reward: 530.1000025942922\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 470000 / 500000\n",
      "average reward: 196.8000004760921 average time: 116.8 best_reward: 443.500001527369\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 293.11500072032214 average time: 161.15 best_reward: 727.5000006631017\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 490000 / 500000\n",
      "average reward: 270.175000552088 average time: 169.8 best_reward: 984.2000020071864\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 239.61500063873828 average time: 131.05 best_reward: 422.4000023826957\n"
     ]
    }
   ],
   "source": [
    "train('./VNet[16]', activation_function = torch.nn.Tanh, grad_norm = 0.5, orthagonal_init=True, arch=[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fd7c94-6682-49d9-a4d2-78b747ecf174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 258.6200002774596 average time: 391.9 best_reward: 415.0000023022294\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 274.4250005282462 average time: 394.75 best_reward: 589.7000009045005\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 278.0500007070601 average time: 279.25 best_reward: 725.6000024750829\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 222.535000718385 average time: 182.75 best_reward: 591.8000022098422\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 229.44000050574542 average time: 173.65 best_reward: 421.80000104755163\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 234.7150006607175 average time: 145.8 best_reward: 419.50000167638063\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 238.550000789389 average time: 123.05 best_reward: 492.100001886487\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 235.66500058732927 average time: 138.4 best_reward: 420.7000007033348\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 194.0500003144145 average time: 111.5 best_reward: 420.50000147521496\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 271.83500066399574 average time: 174.0 best_reward: 446.10000167787075\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 275.06000075004994 average time: 171.75 best_reward: 418.9000014588237\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 234.73000045008956 average time: 148.7 best_reward: 575.6000014469028\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 238.90500052757562 average time: 132.25 best_reward: 420.70000141859055\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 257.13000064343214 average time: 153.0 best_reward: 602.0000017881393\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 290.8250007018447 average time: 176.7 best_reward: 490.70000176131725\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 232.70000038146972 average time: 122.65 best_reward: 420.8000010922551\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 288.19000063464046 average time: 157.9 best_reward: 529.300002232194\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 241.98500059582292 average time: 128.4 best_reward: 575.5000022500753\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 239.3550004441291 average time: 148.3 best_reward: 448.90000089257956\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 248.95500058457256 average time: 153.7 best_reward: 813.5000024065375\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 235.6400005210191 average time: 137.4 best_reward: 420.4000001102686\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 241.19000046662987 average time: 144.65 best_reward: 492.60000128299\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 205.4550004541874 average time: 120.5 best_reward: 487.3999998867512\n",
      "GOAL\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 308.5950007379055 average time: 206.5 best_reward: 527.1000028699636\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 253.4750008672476 average time: 141.3 best_reward: 594.8000014051795\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 213.5850008096546 average time: 125.55 best_reward: 418.500002078712\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 261.3300005786121 average time: 159.55 best_reward: 447.70000004023314\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 267.40500069707633 average time: 146.2 best_reward: 422.3000022992492\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 203.69500025175512 average time: 105.95 best_reward: 447.7000004351139\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 229.80000050626694 average time: 121.7 best_reward: 596.900002323091\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 206.07500052824616 average time: 103.55 best_reward: 491.5000020042062\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 251.48000054098665 average time: 148.25 best_reward: 525.6000018790364\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 260.5750008188188 average time: 138.85 best_reward: 528.700001180172\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 242.50000053606928 average time: 139.9 best_reward: 573.5000003352761\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 231.98500071428717 average time: 126.25 best_reward: 446.80000099539757\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 240.73000075295568 average time: 128.9 best_reward: 492.1000024601817\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 275.28500081785023 average time: 151.95 best_reward: 528.4000014960766\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 264.9250007968396 average time: 148.9 best_reward: 728.9000027254224\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 218.71000058725477 average time: 115.55 best_reward: 447.5000018402934\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 267.3450008202344 average time: 149.75 best_reward: 490.3000008612871\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 213.67500062584878 average time: 114.9 best_reward: 527.9000020995736\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 207.1900005541742 average time: 109.15 best_reward: 415.29999969154596\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 253.15500065088273 average time: 144.5 best_reward: 576.7000018656254\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 251.41500071361662 average time: 123.5 best_reward: 528.6000018641353\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 245.1350007828325 average time: 142.3 best_reward: 525.0000025480986\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 460000 / 500000\n",
      "average reward: 225.3050005953759 average time: 122.65 best_reward: 441.800001449883\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 470000 / 500000\n",
      "average reward: 201.2550006210804 average time: 106.2 best_reward: 420.80000161379576\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 234.94000074453652 average time: 122.85 best_reward: 524.600002348423\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 312.62500083372 average time: 174.0 best_reward: 525.6000014916062\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 268.49500077441337 average time: 142.65 best_reward: 447.70000188052654\n"
     ]
    }
   ],
   "source": [
    "train('./VNet[16, 16]', activation_function = torch.nn.Tanh, grad_norm = 0.5, orthagonal_init=True, arch=[16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "952240a0-c282-4557-9269-d3bf1e5ce192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 10000 / 500000\n",
      "average reward: 250.6292176569812 average time: 166.45 best_reward: 532.5334246493876\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 20000 / 500000\n",
      "average reward: 237.2991449583322 average time: 152.65 best_reward: 534.4387546144426\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 30000 / 500000\n",
      "average reward: 237.30986139168962 average time: 175.35 best_reward: 430.111417805776\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 40000 / 500000\n",
      "average reward: 259.3037021904718 average time: 252.95 best_reward: 584.7044986691326\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 50000 / 500000\n",
      "average reward: 232.39283358668908 average time: 161.0 best_reward: 428.29730000160635\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 60000 / 500000\n",
      "average reward: 297.62827633839333 average time: 250.9 best_reward: 602.1543931867927\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 70000 / 500000\n",
      "average reward: 241.2186571329832 average time: 151.05 best_reward: 741.1945324619301\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 80000 / 500000\n",
      "average reward: 224.85016202640253 average time: 143.0 best_reward: 453.90172897838056\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 90000 / 500000\n",
      "average reward: 266.1031419148436 average time: 166.6 best_reward: 500.0919026695192\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 100000 / 500000\n",
      "average reward: 231.6264807689935 average time: 167.15 best_reward: 428.5244296975434\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 110000 / 500000\n",
      "average reward: 288.0927816993441 average time: 162.65 best_reward: 500.0401382185519\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 120000 / 500000\n",
      "average reward: 249.66461908947676 average time: 134.9 best_reward: 430.1065724901855\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 130000 / 500000\n",
      "average reward: 240.0484564027749 average time: 130.35 best_reward: 743.6379246395081\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 140000 / 500000\n",
      "average reward: 267.7300656203646 average time: 149.0 best_reward: 430.2732178159058\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 150000 / 500000\n",
      "average reward: 212.8407712860033 average time: 113.8 best_reward: 500.18449747748673\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 160000 / 500000\n",
      "average reward: 228.97754310565068 average time: 123.0 best_reward: 536.0463658720255\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 170000 / 500000\n",
      "average reward: 229.57562958411873 average time: 114.65 best_reward: 453.8442925494164\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 180000 / 500000\n",
      "average reward: 309.13852213586216 average time: 174.45 best_reward: 604.4157771002501\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 190000 / 500000\n",
      "average reward: 278.2518577315379 average time: 156.45 best_reward: 533.2924558650702\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 200000 / 500000\n",
      "average reward: 235.72782669919542 average time: 117.15 best_reward: 603.6223508995026\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 210000 / 500000\n",
      "average reward: 235.16591002708302 average time: 118.5 best_reward: 538.2873369026929\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 220000 / 500000\n",
      "average reward: 269.28675776529127 average time: 145.0 best_reward: 455.0914882309735\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 230000 / 500000\n",
      "average reward: 281.3795295500662 average time: 164.2 best_reward: 455.0385464578867\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 240000 / 500000\n",
      "average reward: 281.9340308061801 average time: 156.95 best_reward: 455.8630446959287\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 250000 / 500000\n",
      "average reward: 252.1639097444713 average time: 131.2 best_reward: 500.747196059674\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 260000 / 500000\n",
      "average reward: 255.38418326932006 average time: 142.55 best_reward: 499.4879583232105\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 270000 / 500000\n",
      "average reward: 230.22653631391003 average time: 120.9 best_reward: 427.4309339839965\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 280000 / 500000\n",
      "average reward: 227.14497218080797 average time: 107.4 best_reward: 534.0373700000346\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 290000 / 500000\n",
      "average reward: 281.9367922356352 average time: 139.2 best_reward: 585.1334953680634\n",
      "GOAL\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 300000 / 500000\n",
      "average reward: 231.15222455556506 average time: 122.1 best_reward: 430.1528711486608\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 310000 / 500000\n",
      "average reward: 216.36793741688598 average time: 134.15 best_reward: 426.3343955595046\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 320000 / 500000\n",
      "average reward: 288.2711243405938 average time: 186.25 best_reward: 790.5023526716977\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 330000 / 500000\n",
      "average reward: 228.5780380365439 average time: 127.4 best_reward: 499.39688686840236\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 340000 / 500000\n",
      "average reward: 211.06326617868618 average time: 105.0 best_reward: 454.7995853088796\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 350000 / 500000\n",
      "average reward: 233.47502062460407 average time: 118.85 best_reward: 535.5890652798116\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 360000 / 500000\n",
      "average reward: 261.36147374403663 average time: 145.05 best_reward: 537.2844293639064\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 370000 / 500000\n",
      "average reward: 285.5774566916749 average time: 182.45 best_reward: 500.2683043349534\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 380000 / 500000\n",
      "average reward: 246.3051313912496 average time: 123.65 best_reward: 532.5301053784788\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 390000 / 500000\n",
      "average reward: 202.27645640312693 average time: 101.9 best_reward: 535.8453294429928\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 400000 / 500000\n",
      "average reward: 188.64013804313726 average time: 87.7 best_reward: 584.1982012838125\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 410000 / 500000\n",
      "average reward: 280.13304485860283 average time: 142.95 best_reward: 604.1084400452673\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 420000 / 500000\n",
      "average reward: 191.48492719391362 average time: 95.4 best_reward: 428.2184089682996\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 430000 / 500000\n",
      "average reward: 239.23484409493395 average time: 114.1 best_reward: 535.407267536968\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 440000 / 500000\n",
      "average reward: 234.43232849715278 average time: 122.25 best_reward: 500.27764736302197\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 450000 / 500000\n",
      "average reward: 227.93841133643872 average time: 121.0 best_reward: 739.6963320877403\n",
      "<class 'PPO.PPO'>\n",
      "GOAL\n",
      "time steps: 460000 / 500000\n",
      "average reward: 293.4693839301355 average time: 142.25 best_reward: 1000.7452588472515\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 470000 / 500000\n",
      "average reward: 227.0930449174717 average time: 112.1 best_reward: 537.3706577718258\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 480000 / 500000\n",
      "average reward: 256.00132858753204 average time: 124.5 best_reward: 500.53162740916014\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 490000 / 500000\n",
      "average reward: 268.01947385089005 average time: 140.6 best_reward: 453.9474728386849\n",
      "<class 'PPO.PPO'>\n",
      "time steps: 500000 / 500000\n",
      "average reward: 232.1527953390032 average time: 115.85 best_reward: 499.3076805174351\n"
     ]
    }
   ],
   "source": [
    "train('./yPosReward', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, reward_scheme = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea66de06-611b-4013-bd2c-a7c47365f829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up env...\n",
      "Setting up model...\n",
      "Initializing the predictor network\n",
      "Features 512\n",
      "Action Space Discrete(3)\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./scoreReward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTanh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morthagonal_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_scheme\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ktnelson/MarioRL/env.py:127\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(save_dir, activation_function, orthagonal_init, grad_norm, value_clip, annealing, intrins_reward, feature_extractor, reward_clip, arch, reward_scheme)\u001b[0m\n\u001b[1;32m    121\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs, \n\u001b[1;32m    122\u001b[0m             tensorboard_log\u001b[38;5;241m=\u001b[39msave_dir, learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE, n_steps\u001b[38;5;241m=\u001b[39mN_STEPS,\n\u001b[1;32m    123\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, n_epochs\u001b[38;5;241m=\u001b[39mN_EPOCHS, gamma\u001b[38;5;241m=\u001b[39mGAMMA, gae_lambda\u001b[38;5;241m=\u001b[39mGAE, \n\u001b[1;32m    124\u001b[0m             ent_coef\u001b[38;5;241m=\u001b[39mENT_COEF, clip_range_vf \u001b[38;5;241m=\u001b[39m value_clip, max_grad_norm \u001b[38;5;241m=\u001b[39m grad_norm, annealing \u001b[38;5;241m=\u001b[39m annealing, intrins_reward \u001b[38;5;241m=\u001b[39m intrins_reward)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_TIMESTEP_NUMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ktnelson/MarioRL/PPO.py:336\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    325\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m     reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPPO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ktnelson/MarioRL/on_policy_algorithm.py:357\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    353\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 357\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/home/ktnelson/MarioRL/on_policy_algorithm.py:267\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mNew: Calculate the intrinsic reward and add it to the models rewards\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    266\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs\n\u001b[0;32m--> 267\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintrins_reward:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:48\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]], np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],]:\n\u001b[0;32m---> 48\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackedobs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m/home/ktnelson/MarioRL/env.py:138\u001b[0m, in \u001b[0;36mSkipFrame.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    136\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m--> 138\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m/home/ktnelson/MarioRL/rewards_change.py:67\u001b[0m, in \u001b[0;36mScoreBenefitWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m delta_score \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_score\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_delta_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_delta_score, delta_score)\n\u001b[0;32m---> 67\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdelta_score\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_delta_score\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_pos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_x_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "train('./scoreReward', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, reward_scheme = 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35335233-b812-4abf-9a42-8b205b4197e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('./RewardClip', activation_function = torch.nn.Tanh, orthagonal_init=True, grad_norm = 0.5, reward_clip=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170cef7-eefd-4458-aae6-9a27a1fa957d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
